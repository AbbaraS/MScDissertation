{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install totalsegmentator\n",
    "!pip install dicom2nifti\n",
    "!pip install numpy==1.23.5 --force-reinstall\n",
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mount MyDrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68df21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "from preprocess import *\n",
    "from mydataloader import * \n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22d5b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from totalsegmentator.python_api import totalsegmentator\n",
    "\n",
    "import nibabel as nib\n",
    "from nibabel.orientations import aff2axcodes\n",
    "#from nibabel import Nifti1Image\n",
    "\n",
    "#import dicom2nifti\n",
    "#import dicom2nifti.convert_dicom\n",
    "\n",
    "import SimpleITK as sitk\n",
    "#import scipy.ndimage\n",
    "#from skimage import measure\n",
    "#import imageio\n",
    "\n",
    "\n",
    "#from monai.data.meta_tensor import MetaTensor\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import ListedColormap\n",
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "import re\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "260938af",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal='normal_cases'\n",
    "tako='takotsubo_cases'\n",
    "\n",
    "NORMAL=normal\n",
    "TAKO=tako\n",
    "\n",
    "# folder paths in Colab\n",
    "base_dicom_root = f\"drive/MyDrive/Thesis/Awab/Takotsubo-Syndrome/data/Inputs\"\n",
    "base_input_root = f\"drive/MyDrive/Thesis/Suleima/data/Inputs\"\n",
    "base_output_root = f\"drive/MyDrive/Thesis/Suleima/data/Outputs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3da7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "case=TAKO\n",
    "\n",
    "patientID = \"AG 11370442\"\n",
    "patientID = \"JMB 38365177\"\n",
    "patientID = \"JNMP 22916312\"\n",
    "patientID = \"HSM 3468857\"\n",
    "patientID = \"AMS 22722383\"\n",
    "patientID = \"BFM 10624765\"\n",
    "patientID = \"BLT 11022191\"\n",
    "patientID = \"EIC 26844753\"\n",
    "patientID = \"JDMB 137882\"\n",
    "\n",
    "OG_file_paths[\"LV\"]\n",
    "OG_file_paths[\"RV\"]\n",
    "OG_file_paths[\"LA\"]\n",
    "OG_file_paths[\"RA\"]\n",
    "OG_file_paths[\"MYO\"]\n",
    "OG_file_paths[\"CT\"]\n",
    "\n",
    "cropped_file_paths[\"Mask\"]\n",
    "cropped_file_paths[\"CT\"]\n",
    "cropped_file_paths[\"LV\"]\n",
    "cropped_file_paths[\"RV\"]\n",
    "cropped_file_paths[\"LA\"]\n",
    "cropped_file_paths[\"RA\"]\n",
    "cropped_file_paths[\"MYO\"]\n",
    "\n",
    "'''\n",
    "import os\n",
    "case=TAKO\n",
    "patientID = \"AG 11370442\"\n",
    "\n",
    "base_input_root = f\"data/Inputs/{case}\" #/{patient}\n",
    "base_output_root = f\"data/Outputs/{case}\" #/{patient}\n",
    "input_folder = os.path.join(base_input_root, patientID)\n",
    "output_folder = os.path.join(base_output_root, patientID)\n",
    "png_slices_folder=os.path.join(output_folder, \"png_slices\")\n",
    "\n",
    "cropped_file_paths = get_cropped_file_paths(output_folder)\n",
    "OG_file_paths = get_OG_file_paths(input_folder)\n",
    "\n",
    "#slice_CT(output_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ad33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "\n",
    "class SingleBranchCNN(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(SingleBranchCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, padding=1)  # (B, 16, H, W)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2)                                        # (B, 16, H/2, W/2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)            # (B, 32, H/2, W/2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2)                                        # (B, 32, H/4, W/4)\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(0.3)  # Dropout for feature maps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout(x)\n",
    "        return x  # Output shape: (B, 32, H/4, W/4)\n",
    "\n",
    "class MultiViewCNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, input_size=(65, 65), use_metadata=False):\n",
    "        super(MultiViewCNN, self).__init__()\n",
    "\n",
    "        self.use_metadata = use_metadata\n",
    "        H, W = input_size\n",
    "\n",
    "        # Three branches\n",
    "        self.axial_branch = SingleBranchCNN(in_channels)\n",
    "        self.sagittal_branch = SingleBranchCNN(in_channels)\n",
    "        self.coronal_branch = SingleBranchCNN(in_channels)\n",
    "\n",
    "        # Output feature map size after 2x MaxPool2d(2)\n",
    "        flattened_size = 32 * (H // 4) * (W // 4)  # each branch output flattened\n",
    "\n",
    "        total_features = 3 * flattened_size       # all branches\n",
    "        if use_metadata:\n",
    "            total_features += 2  # e.g. age and gender\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(total_features, 128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 1)  # Binary classification (output: logit)\n",
    "\n",
    "    def forward(self, axial, sagittal, coronal, meta=None):\n",
    "        a = self.axial_branch(axial)\n",
    "        s = self.sagittal_branch(sagittal)\n",
    "        c = self.coronal_branch(coronal)\n",
    "\n",
    "        a = a.view(a.size(0), -1)  # flatten\n",
    "        s = s.view(s.size(0), -1)\n",
    "        c = c.view(c.size(0), -1)\n",
    "\n",
    "        x = torch.cat([a, s, c], dim=1)\n",
    "\n",
    "        if self.use_metadata and meta is not None:\n",
    "            x = torch.cat([x, meta], dim=1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)  # no sigmoid here (use BCEWithLogitsLoss)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc624c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing normal_cases\n",
      "Processing takotsubo_cases\n",
      "Takotsubo: 81, Normal: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sulei\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | Train Loss: 0.9971 | Val Loss: 0.6446 | Train Acc: 52.29% | LR: 0.000100\n",
      "Epoch [2/50] | Train Loss: 0.6598 | Val Loss: 0.5989 | Train Acc: 65.14% | LR: 0.000100\n",
      "Epoch [3/50] | Train Loss: 0.5735 | Val Loss: 0.5771 | Train Acc: 67.89% | LR: 0.000100\n",
      "Epoch [4/50] | Train Loss: 0.5403 | Val Loss: 0.5984 | Train Acc: 72.48% | LR: 0.000100\n",
      "No improvement in val loss (1/3)\n",
      "Epoch [5/50] | Train Loss: 0.4076 | Val Loss: 0.5515 | Train Acc: 82.57% | LR: 0.000100\n",
      "Epoch [6/50] | Train Loss: 0.4270 | Val Loss: 0.5997 | Train Acc: 82.57% | LR: 0.000100\n",
      "No improvement in val loss (1/3)\n",
      "Epoch [7/50] | Train Loss: 0.3281 | Val Loss: 0.4957 | Train Acc: 87.16% | LR: 0.000100\n",
      "Epoch [8/50] | Train Loss: 0.3458 | Val Loss: 0.5179 | Train Acc: 88.07% | LR: 0.000100\n",
      "No improvement in val loss (1/3)\n",
      "Epoch [9/50] | Train Loss: 0.2221 | Val Loss: 0.6800 | Train Acc: 89.91% | LR: 0.000100\n",
      "No improvement in val loss (2/3)\n",
      "Epoch [10/50] | Train Loss: 0.1738 | Val Loss: 0.5386 | Train Acc: 95.41% | LR: 0.000100\n",
      "No improvement in val loss (3/3)\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "metadata, labels, slices = load_dataset()\n",
    "dataset = SliceDataset(\n",
    "    slices_dict=slices,          \n",
    "    metadata_dict=metadata,      \n",
    "    labels_dict=labels           \n",
    ")\n",
    "\n",
    "takotsubo_count = sum(1 for sample in dataset if sample[\"label\"].item() == 1)\n",
    "normal_count = sum(1 for sample in dataset if sample[\"label\"].item() == 0)\n",
    "\n",
    "print(f\"Takotsubo: {takotsubo_count}, Normal: {normal_count}\")\n",
    "\n",
    "dataset_loader = DataLoaderModule(dataset, batch_size=1)\n",
    "\n",
    "# Model setup\n",
    "model = MultiViewCNN(in_channels=3, input_size=(65, 65), use_metadata=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#class_weight = takotsubo_count / normal_count if normal_count > 0 else 1.0\n",
    "pos_weight = torch.tensor([takotsubo_count / normal_count])  \n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
    "train_loader = dataset_loader.get_train_loader()\n",
    "val_loader = dataset_loader.get_val_loader()\n",
    "test_loader = dataset_loader.get_test_loader()\n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "threshold_cutoff = 0.5  # threshold for binary classification\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "num_epochs = 50  # longer max epochs now\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        x_axial = batch[\"axial\"].to(device)\n",
    "        x_sagittal = batch[\"sagittal\"].to(device)\n",
    "        x_coronal = batch[\"coronal\"].to(device)\n",
    "        x_meta = batch[\"meta\"].to(device)\n",
    "        y = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(x_axial, x_sagittal, x_coronal, x_meta)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > threshold_cutoff).float()\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # ====== Validation ======\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_axial = batch[\"axial\"].to(device)\n",
    "            x_sagittal = batch[\"sagittal\"].to(device)\n",
    "            x_coronal = batch[\"coronal\"].to(device)\n",
    "            x_meta = batch[\"meta\"].to(device)\n",
    "            y = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(x_axial, x_sagittal, x_coronal, x_meta)\n",
    "            loss = criterion(outputs, y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Train Acc: {train_acc:.2f}% | LR: {current_lr:.6f}\")\n",
    "    scheduler.step(avg_val_loss)\n",
    "    # ====== Early stopping check ======\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")  # save best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in val loss ({patience_counter}/{patience})\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "# ========== TEST EVALUATION WITH ROC AUC ========== #\n",
    "\n",
    "\n",
    "# Load best model\n",
    "torch.save(model.state_dict(), \"best_model.pt\")\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_prob = []\n",
    "threshold_cutoff = 0.5  # threshold for binary classification\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_axial = batch[\"axial\"].to(device)\n",
    "        x_sagittal = batch[\"sagittal\"].to(device)\n",
    "        x_coronal = batch[\"coronal\"].to(device)\n",
    "        x_meta = batch[\"meta\"].to(device)\n",
    "        y = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(x_axial, x_sagittal, x_coronal, x_meta)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()  # shape (B, 1)\n",
    "\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_prob.extend(probs)\n",
    "\n",
    "# Flatten\n",
    "y_true = [int(x) for x in y_true]\n",
    "y_prob = [float(p) for p in y_prob]\n",
    "y_pred = [int(p > threshold_cutoff) for p in y_prob]\n",
    "\n",
    "# Report\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Normal\", \"Takotsubo\"]))\n",
    "\n",
    "# ROC AUC\n",
    "auc_score = roc_auc_score(y_true, y_prob)\n",
    "print(f\"ðŸŽ¯ ROC AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# ROC Curve plot (optional)\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal\", \"Takotsubo\"])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9142820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, meta):\n",
    "        # Create dummy tensors for image branches\n",
    "        batch_size = meta.size(0)\n",
    "        dummy = torch.zeros((batch_size, 3, 65, 65)).to(meta.device)\n",
    "        return self.model(dummy, dummy, dummy, meta)\n",
    "    \n",
    "class MultiModalWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, axial, sagittal, coronal, meta):\n",
    "        return self.model(axial, sagittal, coronal, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect background (for DeepExplainer) and test examples\n",
    "bg_axial, bg_sagittal, bg_coronal, bg_meta = [], [], [], []\n",
    "test_axial, test_sagittal, test_coronal, test_meta = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(dataset_loader.get_val_loader()):\n",
    "        if i >= 10: break  # Limit background\n",
    "        bg_axial.append(batch[\"axial\"])\n",
    "        bg_sagittal.append(batch[\"sagittal\"])\n",
    "        bg_coronal.append(batch[\"coronal\"])\n",
    "        bg_meta.append(batch[\"meta\"])\n",
    "\n",
    "    for batch in dataset_loader.get_test_loader():\n",
    "        test_axial.append(batch[\"axial\"])\n",
    "        test_sagittal.append(batch[\"sagittal\"])\n",
    "        test_coronal.append(batch[\"coronal\"])\n",
    "        test_meta.append(batch[\"meta\"])\n",
    "\n",
    "# Stack all background and test tensors\n",
    "bg_input = [\n",
    "    torch.cat(bg_axial).to(device),\n",
    "    torch.cat(bg_sagittal).to(device),\n",
    "    torch.cat(bg_coronal).to(device),\n",
    "    torch.cat(bg_meta).to(device),\n",
    "]\n",
    "\n",
    "test_input = [\n",
    "    torch.cat(test_axial).to(device),\n",
    "    torch.cat(test_sagittal).to(device),\n",
    "    torch.cat(test_coronal).to(device),\n",
    "    torch.cat(test_meta).to(device),\n",
    "]\n",
    "\n",
    "#explainer = shap.DeepExplainer(MultiModalWrapper(model), bg_input)\n",
    "#shap_values = explainer.shap_values(test_input, check_additivity=False)\n",
    "\n",
    "explainer = shap.GradientExplainer(MultiModalWrapper(model), bg_input)\n",
    "shap_values = explainer.shap_values(test_input)\n",
    "\n",
    "meta_shap = shap_values[3]               # SHAP values for metadata branch (shape: [n_samples, 2])\n",
    "meta_shap = np.squeeze(meta_shap)\n",
    "meta_input = test_input[3].cpu().numpy() # Original input values for metadata\n",
    "\n",
    "print(\"meta_shap shape:\", meta_shap.shape)       # should be (n_samples, 2)\n",
    "print(\"meta_input shape:\", meta_input.shape)\n",
    "\n",
    "shap.summary_plot(\n",
    "    meta_shap,\n",
    "    features=meta_input,\n",
    "    feature_names=[\"Age\", \"Gender\"]\n",
    ")\n",
    "\n",
    "shap_axial = shap_values[0][:, 0, :, :, 0]       # shape (24, 64, 64)\n",
    "image_axial = test_input[0][:, 0, :, :].cpu().numpy()  # shape (24, 64, 64)\n",
    "print(f\"shap_axial shape: {shap_axial.shape}, image_axial shape: {image_axial.shape}\")\n",
    "shap_coronal = shap_values[0][:, 1, :, :, 0]\n",
    "image_coronal = test_input[1][:, 0, :, :].cpu().numpy()\n",
    "print(f\"shap_coronal shape: {shap_coronal.shape}, image_coronal shape: {image_coronal.shape}\")\n",
    "shap_sagittal = shap_values[0][:, 2, :, :, 0]\n",
    "image_sagittal = test_input[2][:, 0, :, :].cpu().numpy()\n",
    "print(f\"shap_sagittal shape: {shap_sagittal.shape}, image_sagittal shape: {image_sagittal.shape}\")\n",
    "\n",
    "titles = [\"Axial\", \"Coronal\", \"Sagittal\"]\n",
    "for shap_img, input_img, title in zip(\n",
    "    [shap_axial, shap_coronal, shap_sagittal],\n",
    "    [image_axial, image_coronal, image_sagittal],\n",
    "    titles):\n",
    "    print(f\"Plotting SHAP for {title} view\")\n",
    "    shap.image_plot(shap_img, input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "# Load image data\n",
    "ct_img = nib.load(cropped_file_paths[\"CT\"])\n",
    "mask_img = nib.load(cropped_file_paths[\"Mask\"])\n",
    "lv_img = nib.load(cropped_file_paths[\"LV\"])\n",
    "rv_img = nib.load(cropped_file_paths[\"RV\"])\n",
    "la_img = nib.load(cropped_file_paths[\"LA\"])\n",
    "ra_img = nib.load(cropped_file_paths[\"RA\"])\n",
    "myo_img = nib.load(cropped_file_paths[\"MYO\"])\n",
    "ct_cropped = ct_img.get_fdata()\n",
    "mask_data = mask_img.get_fdata()\n",
    "lv_data = lv_img.get_fdata()\n",
    "rv_data = rv_img.get_fdata()\n",
    "la_data = la_img.get_fdata()\n",
    "ra_data = ra_img.get_fdata()\n",
    "myo_data = myo_img.get_fdata()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slices_from_png(png_slices_folder)\n",
    "crop_trim_resample_heart(input_folder, output_folder)\n",
    "plot_slices_masks(png_slices_folder, cropped_file_paths[\"CT\"], cropped_file_paths[\"Mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d0b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original Shape: (512, 512, 311)\n",
      "original Orientation: ('L', 'A', 'S')\n",
      "Cropped Shape: (195, 149, 138)\n",
      "Cropped Orientation: ('R', 'A', 'S')\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "og_lv =  nib.load(OG_file_paths[\"LV\"]).get_fdata()\n",
    "og_rv =  nib.load(OG_file_paths[\"RV\"]).get_fdata()\n",
    "og_la =  nib.load(OG_file_paths[\"LA\"]).get_fdata()\n",
    "og_ra =  nib.load(OG_file_paths[\"RA\"]).get_fdata()\n",
    "og_myo = nib.load(OG_file_paths[\"MYO\"]).get_fdata()\n",
    "og_ct =  nib.load(OG_file_paths[\"CT\"]).get_fdata()\n",
    "\n",
    "ct_cropped = nib.load(cropped_file_paths[\"CT\"]).get_fdata()\n",
    "lv_cropped = nib.load(cropped_file_paths[\"LV\"]).get_fdata()\n",
    "rv_cropped = nib.load(cropped_file_paths[\"RV\"]).get_fdata()\n",
    "la_cropped = nib.load(cropped_file_paths[\"LA\"]).get_fdata()\n",
    "ra_cropped = nib.load(cropped_file_paths[\"RA\"]).get_fdata()\n",
    "myo_cropped = nib.load(cropped_file_paths[\"MYO\"]).get_fdata()\n",
    "mask = nib.load(cropped_file_paths[\"Mask\"]).get_fdata()\n",
    "\n",
    "'''\n",
    "\n",
    "check_original_mask_alignment(ct_path=OG_file_paths[\"CT\"],mask_path=OG_file_paths[\"LV\"],world_mm=-125.8125,slice_axis='z',slice_index=170)\n",
    "\n",
    "ct_img = nib.load(OG_file_paths[\"CT\"])\n",
    "ct_data = ct_img.get_fdata()\n",
    "print(\"original Shape:\", ct_img.shape)\n",
    "print(\"original Orientation:\", aff2axcodes(ct_img.affine))  \n",
    "\n",
    "cropped_img = nib.load(cropped_file_paths[\"CT\"])\n",
    "cropped_data = cropped_img.get_fdata()\n",
    "print(\"Cropped Shape:\", cropped_img.shape)\n",
    "print(\"Cropped Orientation:\", aff2axcodes(cropped_img.affine))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c8ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch Lightning\n",
    "import lightning as L\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19dbf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
